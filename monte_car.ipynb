{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    S: Start\n",
    "    F: Frozen (safe)\n",
    "    H: Hole\n",
    "    G: Goal\n",
    "\n",
    "Environment:\n",
    "\n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "\n",
    "State indices:\n",
    "\n",
    "    0123\n",
    "    4567\n",
    "    89..\n",
    "    ....\n",
    "\n",
    "Action indices:\n",
    "\n",
    "    0: Left\n",
    "    1: Down\n",
    "    2: Right\n",
    "    3: Up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state  = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(action):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print (state,reward)\n",
    "    env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "act(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Given is the following (non optimal) policy π\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "action: 1\n",
      "4 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "action: 1\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "action: 2\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "action: 0\n",
      "8 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "action: 2\n",
      "9 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "action: 0\n",
      "13 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "action: 2\n",
      "14 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "action: 2\n",
      "10 0.0 False {'prob': 0.3333333333333333}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "action: 1\n",
      "11 0.0 True {'prob': 0.3333333333333333}\n",
      "return: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "state  = env.reset()\n",
    "print (state)\n",
    "done = False\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = pi[state]\n",
    "    print (\"action:\", action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print (state, reward, done, info)\n",
    "    if done:\n",
    "        print (\"return:\", reward) # return for all visited states is here last reward\n",
    "\n",
    "env.render(close=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(policy,env=env):\n",
    "    steps = []\n",
    "    state  = env.reset()\n",
    "    #print (state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = policy[state]\n",
    "        old_state_action = [state,action]\n",
    "        #print (\"random action:\", action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps.append(old_state_action +[state, int(reward)])\n",
    "    env.render()\n",
    "    return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 4, 0], [4, 1, 5, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What is the average performance of the policy, i.e. \n",
    "### the percentage that the agent reach the goal state starting from the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of states\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02296805,  0.01945795,  0.03863333,  0.01859107,  0.02594188,\n",
       "        0.        ,  0.07941643,  0.        ,  0.0535373 ,  0.13656958,\n",
       "        0.2423385 ,  0.        ,  0.        ,  0.36415039,  0.60056032,  1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average percentage of policy\n",
    "def compute_average_percentage(policy,nb_of_episodes):\n",
    "    N = np.zeros(state_space_size, dtype=int)\n",
    "    S = np.zeros(state_space_size)\n",
    "    Gs = np.zeros(state_space_size)\n",
    "    \n",
    "    for e in range(nb_of_episodes):\n",
    "        observations_and_reward_list = run_episode(policy)\n",
    "        G = 0.\n",
    "        for old_action, action, new_state, reward in reversed(observations_and_reward_list): \n",
    "            G = reward + gamma * G\n",
    "            N[new_state] += 1\n",
    "            S[new_state] += G \n",
    "            \n",
    "    Gs[N!=0] = S[N!=0]/N[N!=0]\n",
    "    return Gs\n",
    "\n",
    "compute_average_percentage(pi,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated function\n",
    "# return v_s for every visit monte carlo \n",
    "'''\n",
    "def every_visit_monte_carlo(target_state=0, nb_episodes=1000):\n",
    "    \n",
    "    N = np.zeros(state_space_size, dtype=int)\n",
    "    S = np.zeros(state_space_size)\n",
    "    V = np.zeros(state_space_size)\n",
    "                 \n",
    "    for e in range(nb_episodes):\n",
    "        observations_and_reward_list = run_episode(pi)\n",
    "        G = 0.\n",
    "        for old_action, action, new_state, reward in reversed(observations_and_reward_list): \n",
    "            G = reward + gamma * G\n",
    "            N[new_state] += 1\n",
    "            S[new_state] += G \n",
    "            V[new_state] = S[new_state]/N[new_state]\n",
    "                 \n",
    "    return V\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# deprecated function call\n",
    "v_s = every_visit_monte_carlo()\n",
    "print(v_s)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros([state_space_size,4]) # because we have 4 Actions\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "returns = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 2, 2: 1, 3: 0, 4: 1, 6: 1, 8: 2, 9: 0, 10: 1, 13: 2, 14: 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_q_values_every_visit(nb_of_episodes, policy , run_episode):\n",
    "    \n",
    "    for i in range(nb_of_episodes):\n",
    "        observations_and_reward_list = run_episode(policy)\n",
    "    \n",
    "        G = 0. \n",
    "        action_return = dict() \n",
    "        for old_action, action, new_state, reward in reversed(observations_and_reward_list): \n",
    "            G = reward + gamma * G\n",
    "            action_return[old_action] = action, G \n",
    "     \n",
    "        for old_state, (action, G) in action_return.items():\n",
    "            if old_state is not None:\n",
    "                returns[(old_state, action)].append(G) \n",
    "                re_mean = np.array(returns[(old_state, action)]).mean() \n",
    "                Q[(old_state,) + (action,)] = re_mean\n",
    "            \n",
    "                policy[old_state] = np.argmax(Q[(old_state)])\n",
    "                \n",
    "        if(i%50000 == 0.0):\n",
    "            print(\"ok, im doing my work\",i)\n",
    "        \n",
    "    return policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_of_episodes = 50000\n",
    "#policy = compute_q_values_every_visit(nb_of_episodes, pi , run_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_exploring_start(policy, env=env):\n",
    "    steps = []\n",
    "    state  = env.reset()\n",
    "    #print (state)\n",
    "    done = False\n",
    "    start = True\n",
    "    while not done:\n",
    "        if start:\n",
    "            action = np.random.binomial(3, p=0.25)\n",
    "            start = False\n",
    "        else:\n",
    "            action = pi[state]\n",
    "        old_state_action = [state,action]\n",
    "        #print (\"random action:\", action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps.append(old_state_action +[state, int(reward)])\n",
    "    #env.render() \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_epsilon_greedy(policy, epsilon=0.1, env=env):\n",
    "    steps = []\n",
    "    state  = env.reset()\n",
    "    #print (state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() > epsilon:\n",
    "            action = policy[state]\n",
    "        else:\n",
    "            action = np.random.randint(0,4)\n",
    "        old_state_action = [state,action]\n",
    "        #print (\"random action:\", action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps.append(old_state_action +[state, int(reward)])\n",
    "    #env.render() \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_episodes = 500000\n",
    "t0 = time.time()\n",
    "policy = compute_q_values_every_visit(nb_of_episodes, pi , run_episode_epsilon_greedy)\n",
    "print(\"\\nCompute policy took time\",time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOptimal ppolicy ----- \",policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ values ----\\n\\n\",Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy = lambda a: pi[a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g.:\n",
    "#v_s = first_visit_monte_carlo_prediction(target_state=0, nb_episodes=10000)\n",
    "#print (v_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exercise: Monte Carlo Prediction\n",
    "\n",
    "    Computes the q-values of the vistited state-action pairs. qπ(s,a)\n",
    "\n",
    "by Monte Carlo Evaluation (first visit or every visit) with γ=0.99\n",
    "\n",
    ". Implement an appropriate python function.\n",
    "\n",
    "Compute from qπ(s,a)\n",
    "the values of vπ(s)\n",
    "\n",
    "    for all states that have been visited.\n",
    "\n",
    "Exercise: Monte Carlo Control\n",
    "\n",
    "    Modify the policy π\n",
    "\n",
    "to a ϵ-greedy policy and use policy improvement to find an optimal ϵ-greedy policy (in combination with the Monte-Carlo policy evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exercise\n",
    "\n",
    "Compute the state-values vπ(s)\n",
    "of your optimal ϵ-greedy policy π from qπ(s,a)\n",
    "\n",
    ":\n",
    "\n",
    "    Give an mathematical expression for the computation.\n",
    "        Compute the numerical values for vπ(s)\n",
    "\n",
    "from q(s,a) (with numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pi = {0: 2, 1: 3, 2: 0, 3: 3, 4: 0, 6: 0, 8: 3, 9: 1, 10: 0, 13: 2, 14: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 2, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 0, 0],\n",
       " [0, 2, 1, 0],\n",
       " [1, 3, 1, 0],\n",
       " [1, 3, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 1, 0],\n",
       " [1, 3, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 8, 0],\n",
       " [8, 3, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 8, 0],\n",
       " [8, 3, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 8, 0],\n",
       " [8, 3, 8, 0],\n",
       " [8, 3, 4, 0],\n",
       " [4, 0, 8, 0],\n",
       " [8, 3, 8, 0],\n",
       " [8, 3, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 0, 0],\n",
       " [0, 2, 0, 0],\n",
       " [0, 2, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 4, 0],\n",
       " [4, 0, 8, 0],\n",
       " [8, 3, 8, 0],\n",
       " [8, 3, 9, 0],\n",
       " [9, 1, 10, 0],\n",
       " [10, 0, 14, 0],\n",
       " [14, 1, 13, 0],\n",
       " [13, 2, 13, 0],\n",
       " [13, 2, 9, 0],\n",
       " [9, 1, 10, 0],\n",
       " [10, 0, 14, 0],\n",
       " [14, 1, 15, 1]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode(new_pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
